{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0947d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# script for calculating the negative log-likelihood for 16-cluster set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f15f9a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import multivariate_normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f020124",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('clus16/cluster_1.csv', header = None)\n",
    "df2 = pd.read_csv('clus16/cluster_2.csv', header = None)\n",
    "df3 = pd.read_csv('clus16/cluster_3.csv', header = None)\n",
    "df4 = pd.read_csv('clus16/cluster_4.csv', header = None)\n",
    "df5 = pd.read_csv('clus16/cluster_5.csv', header = None)\n",
    "df6 = pd.read_csv('clus16/cluster_6.csv', header = None)\n",
    "df7 = pd.read_csv('clus16/cluster_7.csv', header = None)\n",
    "df8 = pd.read_csv('clus16/cluster_8.csv', header = None)\n",
    "df9 = pd.read_csv('clus16/cluster_9.csv', header = None)\n",
    "df10 = pd.read_csv('clus16/cluster_10.csv', header = None)\n",
    "df11 = pd.read_csv('clus16/cluster_11.csv', header = None)\n",
    "df12 = pd.read_csv('clus16/cluster_12.csv', header = None)\n",
    "df13 = pd.read_csv('clus16/cluster_13.csv', header = None)\n",
    "df14 = pd.read_csv('clus16/cluster_14.csv', header = None)\n",
    "df15 = pd.read_csv('clus16/cluster_15.csv', header = None)\n",
    "df16 = pd.read_csv('clus16/cluster_16.csv', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed42d26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler() \n",
    "X_scaled_1 = scaler.fit_transform(df1)\n",
    "X_scaled_2 = scaler.fit_transform(df2)\n",
    "X_scaled_3 = scaler.fit_transform(df3)\n",
    "X_scaled_4 = scaler.fit_transform(df4)\n",
    "X_scaled_5 = scaler.fit_transform(df5)\n",
    "X_scaled_6 = scaler.fit_transform(df6)\n",
    "X_scaled_7 = scaler.fit_transform(df7)\n",
    "X_scaled_8 = scaler.fit_transform(df8)\n",
    "X_scaled_9 = scaler.fit_transform(df9)\n",
    "X_scaled_10 = scaler.fit_transform(df10)\n",
    "X_scaled_11 = scaler.fit_transform(df11)\n",
    "X_scaled_12 = scaler.fit_transform(df12)\n",
    "X_scaled_13 = scaler.fit_transform(df13)\n",
    "X_scaled_14 = scaler.fit_transform(df14)\n",
    "X_scaled_15 = scaler.fit_transform(df15)\n",
    "X_scaled_16 = scaler.fit_transform(df16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58ca9e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca1 = PCA(n_components=16, random_state=0)\n",
    "pca2 = PCA(n_components=16, random_state=0)\n",
    "pca3 = PCA(n_components=16, random_state=0)\n",
    "pca4 = PCA(n_components=16, random_state=0)\n",
    "pca5 = PCA(n_components=16, random_state=0)\n",
    "pca6 = PCA(n_components=16, random_state=0)\n",
    "pca7 = PCA(n_components=16, random_state=0)\n",
    "pca8 = PCA(n_components=16, random_state=0)\n",
    "pca9 = PCA(n_components=16, random_state=0)\n",
    "pca10 = PCA(n_components=16, random_state=0)\n",
    "pca11 = PCA(n_components=16, random_state=0)\n",
    "pca12 = PCA(n_components=16, random_state=0)\n",
    "pca13 = PCA(n_components=16, random_state=0)\n",
    "pca14 = PCA(n_components=16, random_state=0)\n",
    "pca15 = PCA(n_components=16, random_state=0)\n",
    "pca16 = PCA(n_components=16, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f3336a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_1 = pca1.fit_transform(X_scaled_1)\n",
    "X_pca_2 = pca2.fit_transform(X_scaled_2)\n",
    "X_pca_3 = pca3.fit_transform(X_scaled_3)\n",
    "X_pca_4 = pca4.fit_transform(X_scaled_4)\n",
    "X_pca_5 = pca5.fit_transform(X_scaled_5)\n",
    "X_pca_6 = pca6.fit_transform(X_scaled_6)\n",
    "X_pca_7 = pca7.fit_transform(X_scaled_7)\n",
    "X_pca_8 = pca8.fit_transform(X_scaled_8)\n",
    "X_pca_9 = pca9.fit_transform(X_scaled_9)\n",
    "X_pca_10 = pca10.fit_transform(X_scaled_10)\n",
    "X_pca_11 = pca11.fit_transform(X_scaled_11)\n",
    "X_pca_12 = pca12.fit_transform(X_scaled_12)\n",
    "X_pca_13 = pca13.fit_transform(X_scaled_13)\n",
    "X_pca_14 = pca14.fit_transform(X_scaled_14)\n",
    "X_pca_15 = pca15.fit_transform(X_scaled_15)\n",
    "X_pca_16 = pca16.fit_transform(X_scaled_16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d04378a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd1 = 0.7413 * (np.percentile(X_pca_1[:, 0], 75) - np.percentile(X_pca_1[:, 0], 25))\n",
    "sd2 = 0.7413 * (np.percentile(X_pca_1[:, 1], 75) - np.percentile(X_pca_1[:, 1], 25))\n",
    "sd3 = 0.7413 * (np.percentile(X_pca_1[:, 2], 75) - np.percentile(X_pca_1[:, 2], 25))\n",
    "sd4 = 0.7413 * (np.percentile(X_pca_1[:, 3], 75) - np.percentile(X_pca_1[:, 3], 25))\n",
    "sd5 = 0.7413 * (np.percentile(X_pca_1[:, 4], 75) - np.percentile(X_pca_1[:, 4], 25))\n",
    "sd6 = 0.7413 * (np.percentile(X_pca_1[:, 5], 75) - np.percentile(X_pca_1[:, 5], 25))\n",
    "sd7 = 0.7413 * (np.percentile(X_pca_1[:, 6], 75) - np.percentile(X_pca_1[:, 6], 25))\n",
    "sd8 = 0.7413 * (np.percentile(X_pca_1[:, 7], 75) - np.percentile(X_pca_1[:, 7], 25))\n",
    "sd9 = 0.7413 * (np.percentile(X_pca_1[:, 8], 75) - np.percentile(X_pca_1[:, 8], 25))\n",
    "sd10 = 0.7413 * (np.percentile(X_pca_1[:, 9], 75) - np.percentile(X_pca_1[:, 9], 25))\n",
    "sd11 = 0.7413 * (np.percentile(X_pca_1[:, 10], 75) - np.percentile(X_pca_1[:, 10], 25))\n",
    "sd12 = 0.7413 * (np.percentile(X_pca_1[:, 11], 75) - np.percentile(X_pca_1[:, 11], 25))\n",
    "sd13 = 0.7413 * (np.percentile(X_pca_1[:, 12], 75) - np.percentile(X_pca_1[:, 12], 25))\n",
    "sd14 = 0.7413 * (np.percentile(X_pca_1[:, 13], 75) - np.percentile(X_pca_1[:, 13], 25))\n",
    "sd15 = 0.7413 * (np.percentile(X_pca_1[:, 14], 75) - np.percentile(X_pca_1[:, 14], 25))\n",
    "sd16 = 0.7413 * (np.percentile(X_pca_1[:, 15], 75) - np.percentile(X_pca_1[:, 15], 25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84c43605",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([sd1, sd2, sd3, sd4, sd5, sd6, sd7, sd8, sd9, sd10, sd11, sd12, sd13, sd14, sd15, sd16])\n",
    "arr_diag = np.diag(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b33d1b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = [X_pca_1, X_pca_2, X_pca_3, X_pca_4, X_pca_5, X_pca_6, X_pca_7, X_pca_8, X_pca_9, X_pca_10, X_pca_11, X_pca_12, X_pca_13,\n",
    "      X_pca_14, X_pca_15, X_pca_16]\n",
    "collect_sd = []\n",
    "for xx in range(16):\n",
    "    for yy in range(16):\n",
    "        sd = 0.7413 * (np.percentile(cc[xx][:, yy], 75) - np.percentile(cc[xx][:, yy], 25))\n",
    "        collect_sd.append(sd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f5a016d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_all_sd = [collect_sd[0:16], collect_sd[16:32], collect_sd[32:48], collect_sd[48:64], collect_sd[64:80],\n",
    "                 collect_sd[80:96], collect_sd[96:112], collect_sd[112:128], collect_sd[128:144], collect_sd[144:160],\n",
    "                 collect_sd[160:176], collect_sd[176:192], collect_sd[192:208], collect_sd[208:224], collect_sd[224:240],\n",
    "                 collect_sd[240:256]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1d793c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "PC1 = np.median(X_pca_1[:, 0])\n",
    "PC2 = np.median(X_pca_1[:, 1])\n",
    "PC3 = np.median(X_pca_1[:, 2])\n",
    "PC4 = np.median(X_pca_1[:, 3])\n",
    "PC5 = np.median(X_pca_1[:, 4])\n",
    "PC6 = np.median(X_pca_1[:, 5])\n",
    "PC7 = np.median(X_pca_1[:, 6])\n",
    "PC8 = np.median(X_pca_1[:, 7])\n",
    "PC9 = np.median(X_pca_1[:, 8])\n",
    "PC10 = np.median(X_pca_1[:, 9])\n",
    "PC11 = np.median(X_pca_1[:, 10])\n",
    "PC12 = np.median(X_pca_1[:, 11])\n",
    "PC13 = np.median(X_pca_1[:, 12])\n",
    "PC14 = np.median(X_pca_1[:, 13])\n",
    "PC15 = np.median(X_pca_1[:, 14])\n",
    "PC16 = np.median(X_pca_1[:, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa13b520",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.39742202e+00,  7.66325708e-02,  9.03623968e-01, -3.26769830e-01,\n",
       "        2.23259043e-01, -4.03063114e-02,  3.42049291e-02,  1.04021432e-02,\n",
       "        5.48524040e-02,  1.63423634e-01,  9.16674595e-02, -8.31381115e-02,\n",
       "       -6.59480377e-02,  2.44188441e-02,  3.54378061e-02, -3.67611393e-04])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median = np.array([PC1, PC2, PC3, PC4, PC5, PC6, PC7, PC8, PC9, PC10, PC11, PC12, PC13, PC14, PC15, PC16])\n",
    "median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8572fe58",
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_median = []\n",
    "for pp in range(16):\n",
    "    for qq in range(16):\n",
    "        PC = np.median(cc[pp][:, qq])\n",
    "        collect_median.append(PC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb68feaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gather_all_median = [collect_median[0:16], collect_median[16:32], collect_median[32:48], collect_median[48:64],\n",
    "                     collect_median[64:80], collect_median[80:96], collect_median[96:112], collect_median[112:128],\n",
    "                     collect_median[128:144], collect_median[144:160], collect_median[160:176], collect_median[176:192],\n",
    "                     collect_median[192:208], collect_median[208:224], collect_median[224:240], collect_median[240:256]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfefda0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Likelihood of cluster 1: -24322.038753832152\n",
      "Likelihood of cluster 2: -17411.855388397762\n",
      "Likelihood of cluster 3: -24851.05475268576\n",
      "Likelihood of cluster 4: -23954.217844415132\n",
      "Likelihood of cluster 5: -23941.401795717586\n",
      "Likelihood of cluster 6: -24851.074154565787\n",
      "Likelihood of cluster 7: -22658.544061855424\n",
      "Likelihood of cluster 8: -27054.18960037982\n",
      "Likelihood of cluster 9: -26057.517926618428\n",
      "Likelihood of cluster 10: -26846.552391566926\n",
      "Likelihood of cluster 11: -22562.623687561292\n",
      "Likelihood of cluster 12: -24735.29482518814\n",
      "Likelihood of cluster 13: -19977.575122089343\n",
      "Likelihood of cluster 14: -22415.19232282291\n",
      "Likelihood of cluster 15: -20373.05417465548\n",
      "Likelihood of cluster 16: -26043.775161092897\n"
     ]
    }
   ],
   "source": [
    "kk = []\n",
    "for rr in range(16):\n",
    "    l = multivariate_normal.logpdf(cc[rr], mean = np.array(gather_all_median[rr]), cov = np.diag(gather_all_sd[rr]))\n",
    "    kk.append(sum(l))\n",
    "    print(f'Likelihood of cluster {rr + 1}:', kk[rr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2467215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-378056"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_llh = sum(kk)\n",
    "round(t_llh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c812f380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative Log-Likelihood (NLL) for the 16-cluster set: 378056\n"
     ]
    }
   ],
   "source": [
    "print('Negative Log-Likelihood (NLL) for the 16-cluster set:', round(-t_llh))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
