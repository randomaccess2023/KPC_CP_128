{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e9d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "import configparser\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.transforms import RandomAffine\n",
    "from torchsummary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025d687a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device is:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b6081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stamp for model saving\n",
    "\n",
    "stamp = datetime.datetime.now().strftime('%Y%m%d%H%M')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ad7529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions for 'marginal entropy', 'conditional entropy' and 'KL divergence'\n",
    "\n",
    "def get_entropy_1D(xxx): # marginal entropy\n",
    "    return (-torch.sum(xxx * torch.log(xxx + 1e-8)))\n",
    "def get_entropy_2D(xxx): # conditional entropy\n",
    "    return (-torch.sum(xxx * torch.log(xxx + 1e-8), dim = 1))\n",
    "def get_KLD_1D(ppp, qqq, batch_mean = True): # KL divergence\n",
    "    tmp = torch.sum(ppp * torch.log(ppp + 1e-8) - ppp * torch.log(qqq + 1e-8), dim = 1)\n",
    "    if (batch_mean):\n",
    "        return (torch.mean(tmp))\n",
    "    else:\n",
    "        return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae3d738",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 'reconstruction loss'\n",
    "\n",
    "get_MSELoss = nn.MSELoss(reduction = 'mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e696e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions for plotting the learning curve\n",
    "\n",
    "class history():\n",
    "    def __init__(self, keys):\n",
    "        self.values = {}\n",
    "        for kk in keys:\n",
    "            self.values[kk] = []\n",
    "        self.keys = keys\n",
    "    def append(self, dict_hist):\n",
    "        for kk in dict_hist.keys():\n",
    "            self.values[kk].append(dict_hist[kk])\n",
    "    def mean(self, keys = None):\n",
    "        if (keys is None):\n",
    "            keys = self.keys\n",
    "        mm = {}\n",
    "        for kk in keys:\n",
    "            mm[kk] = np.round(np.mean(self.values[kk]), 3)\n",
    "        return mm\n",
    "    def __getitem__(self, key):\n",
    "        return (self.values[key])\n",
    "    def __str__(self):\n",
    "        tmp = self.mean(self.keys)\n",
    "        return ('\\t'.join([kk + ':' + str(tmp[kk]) for kk in self.keys]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a0f5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config['CNN'] = {'rand_seed': 765, 'ks': 3, 'nf0': 15, 'nf1': 45, 'nf2': 128, 'nf3': 196, 'nf4': 128, 'nf5': 128, 'nf6': 128,\n",
    "                 'nfc': 16}\n",
    "config['IMSAT'] = {'lambda_affine': 0.03, 'lambda_autoencoder': 1.0, 'lambda_entropy_marginal': 0.1,\n",
    "                   'lambda_entropy_mean': 0.03, 'learning_rate': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e68d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_seed = int(config['CNN']['rand_seed'])\n",
    "ks        = int(config['CNN']['ks'])\n",
    "nf0       = int(config['CNN']['nf0'])\n",
    "nf1       = int(config['CNN']['nf1'])\n",
    "nf2       = int(config['CNN']['nf2'])\n",
    "nf3       = int(config['CNN']['nf3'])\n",
    "nf4       = int(config['CNN']['nf4'])\n",
    "nf5       = int(config['CNN']['nf5'])\n",
    "nf6       = int(config['CNN']['nf6'])\n",
    "nfc       = int(config['CNN']['nfc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d84ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_affine           = float(config['IMSAT']['lambda_affine'])\n",
    "lambda_autoencoder      = float(config['IMSAT']['lambda_autoencoder'])\n",
    "lambda_entropy_marginal = float(config['IMSAT']['lambda_entropy_marginal'])\n",
    "lambda_entropy_mean     = float(config['IMSAT']['lambda_entropy_mean'])\n",
    "learning_rate           = float(config['IMSAT']['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65584b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed\n",
    "\n",
    "print('random seed:', rand_seed)\n",
    "\n",
    "torch.manual_seed(rand_seed)\n",
    "torch.cuda.manual_seed(rand_seed)\n",
    "np.random.seed(rand_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b59a328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "\n",
    "data_src = np.load('/project/dsc-is/nono/Documents/kpc/dat0/slice128_Block2_11K.npy')\n",
    "print(data_src.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548d030b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# take the average of RGB values on the channel axis\n",
    "\n",
    "np.round(np.mean(data_src, axis = (0, 1, 2, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbdae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom functions to extract batches of samples\n",
    "\n",
    "def get_batch_index_iii(iii, batch_size = None, shuffle = True):\n",
    "    if (shuffle):\n",
    "        np.random.shuffle(iii)\n",
    "    if (batch_size is not None):\n",
    "        n_batch = len(iii) // batch_size\n",
    "    batch_list = np.array_split(iii, n_batch)\n",
    "    return batch_list\n",
    "def get_batch_index_nn(nn, batch_size = None, shuffle = True):\n",
    "    iii = np.arange(nn)\n",
    "    batch_list = get_batch_index_iii(iii, batch_size = batch_size, shuffle = shuffle)\n",
    "    return batch_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129d276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix, iy = 64, 64 # 128x128 patches are getting rescaled\n",
    "\n",
    "# affine transformation\n",
    "add_random_affine = RandomAffine(degrees = 5, translate = (0.05, 0.05), scale = (0.95, 1.05), fill = (161, 138, 172))\n",
    "def generate_batch(iii, src, device, random_affine = False):\n",
    "    if (random_affine):\n",
    "        tmp = np.empty((len(iii), ix, iy, nf0))\n",
    "        for aa, ii in enumerate(iii):\n",
    "            img_tmp0 = Image.fromarray(src[ii, 0])\n",
    "            img_tmp0 = add_random_affine(img_tmp0).resize((ix, iy)) # HE\n",
    "            img_tmp4 = Image.fromarray(src[ii, 4])\n",
    "            img_tmp4 = add_random_affine(img_tmp4).resize((ix, iy)) # CD31\n",
    "            img_tmp5 = Image.fromarray(src[ii, 5])\n",
    "            img_tmp5 = add_random_affine(img_tmp5).resize((ix, iy)) # CK19\n",
    "            img_tmp6 = Image.fromarray(src[ii, 6])\n",
    "            img_tmp6 = add_random_affine(img_tmp6).resize((ix, iy)) # Ki67\n",
    "            img_tmp7 = Image.fromarray(src[ii, 7])\n",
    "            img_tmp7 = add_random_affine(img_tmp7).resize((ix, iy)) # MT\n",
    "            tmp[aa] = np.concatenate((img_tmp0,img_tmp4, img_tmp5, img_tmp6, img_tmp7), axis = 2)\n",
    "        xxx = torch.tensor(tmp / 256.0, dtype = torch.float32).permute(0, 3, 2, 1)\n",
    "    else:\n",
    "        tmp = np.empty((len(iii), ix, iy, nf0))\n",
    "        for aa, ii in enumerate(iii):\n",
    "            img_tmp0 = Image.fromarray(src[ii, 0]).resize((ix, iy)) # HE\n",
    "            img_tmp4 = Image.fromarray(src[ii, 4]).resize((ix, iy)) # CD31\n",
    "            img_tmp5 = Image.fromarray(src[ii, 5]).resize((ix, iy)) # CK19\n",
    "            img_tmp6 = Image.fromarray(src[ii, 6]).resize((ix, iy)) # Ki67\n",
    "            img_tmp7 = Image.fromarray(src[ii, 7]).resize((ix, iy)) # MT\n",
    "            tmp[aa] = np.concatenate((img_tmp0,img_tmp4, img_tmp5, img_tmp6, img_tmp7), axis = 2)\n",
    "        xxx = torch.tensor(tmp / 256.0, dtype = torch.float32).permute(0, 3, 2, 1)\n",
    "    return (xxx.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f720a3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build encoder architecture\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = nn.Sequential()\n",
    "        self.encoder.add_module('conv1', nn.Conv2d(nf0, nf1, kernel_size = 4, stride = 2, padding = 1))\n",
    "        self.encoder.add_module('bnor1', nn.BatchNorm2d(nf1, affine = True, track_running_stats = True))\n",
    "        self.encoder.add_module('relu1', nn.LeakyReLU(0.1, inplace = True))\n",
    "        self.encoder.add_module('conv2', nn.Conv2d(nf1, nf2, kernel_size = 4, stride = 2, padding = 1))\n",
    "        self.encoder.add_module('bnor2', nn.BatchNorm2d(nf2, affine = True, track_running_stats = True))\n",
    "        self.encoder.add_module('relu2', nn.LeakyReLU(0.1, inplace = True))\n",
    "        self.encoder.add_module('conv3', nn.Conv2d(nf2, nf3, kernel_size = 4, stride = 2, padding = 1))\n",
    "        self.encoder.add_module('bnor3', nn.BatchNorm2d(nf3, affine = True, track_running_stats = True))\n",
    "        self.encoder.add_module('relu3', nn.LeakyReLU(0.1, inplace = True))\n",
    "        self.encoder.add_module('conv4', nn.Conv2d(nf3, nf4, kernel_size = 4, stride = 2, padding = 1))\n",
    "        self.encoder.add_module('bnor4', nn.BatchNorm2d(nf4, affine = True, track_running_stats = True))\n",
    "        self.encoder.add_module('relu4', nn.LeakyReLU(0.1, inplace = True))\n",
    "    def forward(self, xxx):\n",
    "        hhh = self.encoder(xxx)\n",
    "        return hhh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ac024e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build classifier architecture\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.classifier = nn.Sequential()\n",
    "        self.classifier.add_module('conv1', nn.Conv2d(nf4, nf5, kernel_size = 4, stride = 1, padding = 0))\n",
    "        self.classifier.add_module('bnor1', nn.BatchNorm2d(nf5, affine = True, track_running_stats = True))\n",
    "        self.classifier.add_module('relu1', nn.LeakyReLU(0.1, inplace = True))\n",
    "        self.classifier.add_module('conv2', nn.Conv2d(nf5, nf6, kernel_size = 1, stride = 1, padding = 0))\n",
    "        self.classifier.add_module('relu2', nn.LeakyReLU(0.1, inplace = True))\n",
    "        self.classifier.add_module('conv3', nn.Conv2d(nf6, nfc, kernel_size = 1, stride = 1, padding = 0))\n",
    "        self.classifier.add_module('relu3', nn.LeakyReLU(0.1, inplace = True))\n",
    "    def forward(self, hhh):\n",
    "        vvv = self.classifier(hhh)\n",
    "        return vvv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52878a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build decoder architecture\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.add_module('upsm4', nn.UpsamplingBilinear2d(scale_factor = 2))\n",
    "        self.decoder.add_module('dcov4', nn.Conv2d(nf4 + nfc, nf3, kernel_size = 3, stride = 1, padding = 1))\n",
    "        self.decoder.add_module('norm4', nn.BatchNorm2d(nf3, affine = True, track_running_stats = True))\n",
    "        self.decoder.add_module('relu4', nn.LeakyReLU(0.1, inplace = True))\n",
    "        self.decoder.add_module('upsm3', nn.UpsamplingBilinear2d(scale_factor = 2))\n",
    "        self.decoder.add_module('dcov3', nn.Conv2d(nf3, nf2, kernel_size = 3, stride = 1, padding = 1))\n",
    "        self.decoder.add_module('norm3', nn.BatchNorm2d(nf2, affine = True, track_running_stats = True))\n",
    "        self.decoder.add_module('relu3', nn.LeakyReLU(0.1, inplace = True))\n",
    "        self.decoder.add_module('upsm2', nn.UpsamplingBilinear2d(scale_factor = 2))\n",
    "        self.decoder.add_module('dcov2', nn.Conv2d(nf2, nf1, kernel_size = 3, stride = 1, padding = 1))\n",
    "        self.decoder.add_module('norm2', nn.BatchNorm2d(nf1, affine = True, track_running_stats = True))\n",
    "        self.decoder.add_module('relu2', nn.LeakyReLU(0.1, inplace = True))\n",
    "        self.decoder.add_module('upsm1', nn.UpsamplingBilinear2d(scale_factor = 2))\n",
    "        self.decoder.add_module('dcov1', nn.Conv2d(nf1, nf0, kernel_size = 3, stride = 1, padding = 1))\n",
    "        self.decoder.add_module('norm1', nn.BatchNorm2d(nf0, affine = True, track_running_stats = True))\n",
    "        self.decoder.add_module('relu1', nn.Sigmoid())\n",
    "    def forward(self, hhh, vvv):\n",
    "        ccc = vvv.repeat((1, 1, ix // 16, iy // 16))\n",
    "        hhh = torch.cat((hhh, ccc), dim = 1)\n",
    "        yyy = self.decoder(hhh)\n",
    "        return yyy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d18981e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize encoder architecture\n",
    "\n",
    "summary(Encoder().to(device), (nf0, ix, iy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dddccf8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize classifier architecture\n",
    "\n",
    "summary(Classifier().to(device), (nf4, ix // 16, iy // 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cb5485",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize decoder architecture\n",
    "\n",
    "summary(Decoder().to(device).decoder, (nf4 + nfc, ix // 16, iy // 16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1f4ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up 'models' and 'optimizer'\n",
    "\n",
    "model_en = Encoder().to(device)\n",
    "model_cl = Classifier().to(device)\n",
    "model_de = Decoder().to(device)\n",
    "optim_en = optim.Adadelta(model_en.parameters(), lr = learning_rate)\n",
    "optim_cl = optim.Adadelta(model_cl.parameters(), lr = learning_rate)\n",
    "optim_de = optim.Adadelta(model_de.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e6a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to train the model\n",
    "\n",
    "t0 = 0\n",
    "key_loss = ['loss', 'loss_rec', 'entropy_marginal', 'entropy_mean', 'loss_affine']\n",
    "loss_hist = history(['tt'] + key_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb34702",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_epoch = 5000\n",
    "t_print = 10\n",
    "t_log = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ce4d05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# training loop\n",
    "\n",
    "for tt in range(t0, t0 + t_epoch):\n",
    "    loss_tt = history(key_loss)\n",
    "    iii_batch = get_batch_index_nn(10000, batch_size = 100, shuffle = True)\n",
    "    for iii in iii_batch:\n",
    "        xxx_tmp = generate_batch(iii, data_src, device)\n",
    "        xxa_tmp = generate_batch(iii, data_src, device, random_affine = True)\n",
    "        model_en.train()\n",
    "        model_cl.train()\n",
    "        model_de.train()\n",
    "        hhh_tmp = model_en(xxx_tmp)\n",
    "        vvv_tmp = model_cl(hhh_tmp)\n",
    "        yyy_tmp = model_de(hhh_tmp, vvv_tmp)\n",
    "        loss_rec = get_MSELoss(xxx_tmp, yyy_tmp)\n",
    "        ppp_tmp = F.softmax(vvv_tmp.reshape((-1, nfc)), dim = 1)\n",
    "        ppp_mean = torch.mean(ppp_tmp, dim = 0, keepdim = True)\n",
    "        entropy_marginal = get_entropy_1D(ppp_mean)\n",
    "        entropy_mean = torch.mean(get_entropy_2D(ppp_tmp))\n",
    "        hha_tmp = model_en(xxa_tmp)\n",
    "        vva_tmp = model_cl(hha_tmp)\n",
    "        ppa_tmp = F.softmax(vva_tmp.reshape((-1, nfc)), dim = 1)\n",
    "        loss_affine = get_KLD_1D(ppp_tmp, ppa_tmp)\n",
    "        loss_tmp = lambda_autoencoder * loss_rec - lambda_entropy_marginal * entropy_marginal + \\\n",
    "                   lambda_entropy_mean * entropy_mean + lambda_affine * loss_affine\n",
    "        optim_en.zero_grad()\n",
    "        optim_cl.zero_grad()\n",
    "        optim_de.zero_grad()\n",
    "        loss_tmp.backward()\n",
    "        optim_en.step()\n",
    "        optim_cl.step()\n",
    "        optim_de.step()\n",
    "        loss_tt.append({'loss': loss_tmp.item(), 'loss_rec': loss_rec.item(), 'entropy_marginal': entropy_marginal.item(),\n",
    "                        'entropy_mean': entropy_mean.item(), 'loss_affine': loss_affine.item()})\n",
    "    if (tt + 1) % t_log == 0:\n",
    "        loss_hist.append({'tt': tt})\n",
    "        loss_hist.append(loss_tt.mean())\n",
    "    if (tt + 1) % t_print == 0:\n",
    "        print(tt + 1, '/', t0 + t_epoch, '\\t', str(loss_tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e219e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the learning curve\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(loss_hist['tt'], loss_hist['loss'], 'r', label='Loss function', linewidth=3)\n",
    "plt.plot(loss_hist['tt'], loss_hist['loss_rec'], 'g', label='Reconstruction loss', linewidth=3)\n",
    "plt.plot(loss_hist['tt'], lambda_entropy_marginal * np.array(loss_hist['entropy_marginal']) - lambda_entropy_mean * \\\n",
    "         np.array(loss_hist['entropy_mean']), 'b', label='Mutual information (MI)', linewidth=3)\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f77e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check  different shapes\n",
    "\n",
    "print('Encoder input shape\\t:', xxx_tmp.shape)\n",
    "print('\\nEncoder output shape\\t:', hhh_tmp.shape)\n",
    "print('\\nClassifier output shape\\t:', vvv_tmp.shape)\n",
    "print('\\nDecoder output shape\\t:', yyy_tmp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab000f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# folder to save models\n",
    "\n",
    "dir_save = 'models16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7faaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_hist = os.path.join(dir_save, f'hist_modelS_{stamp}_{tt + 1}.tsv')\n",
    "print('saving', path_hist) # save training history\n",
    "tmp = pd.DataFrame.from_dict(loss_hist.values)\n",
    "tmp.to_csv(path_hist, sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cff5d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models\n",
    "\n",
    "path_model_en = os.path.join(dir_save, f'model_en_{stamp}_{tt + 1}.ckpt')\n",
    "path_model_cl = os.path.join(dir_save, f'model_cl_{stamp}_{tt + 1}.ckpt')\n",
    "path_model_de = os.path.join(dir_save, f'model_de_{stamp}_{tt + 1}.ckpt')\n",
    "print('saving', path_model_en)\n",
    "torch.save(model_en.state_dict(), path_model_en)\n",
    "print('saving', path_model_cl)\n",
    "torch.save(model_cl.state_dict(), path_model_cl)\n",
    "print('saving', path_model_de)\n",
    "torch.save(model_de.state_dict(), path_model_de)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
